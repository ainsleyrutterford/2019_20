\documentclass[12pt]{article}
\usepackage{amsfonts, epsfig}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lfoot{\texttt{comsm0034.github.io}}
\lhead{IP\&B - 3\_differential\_entropy - Conor}
\rhead{\thepage}
\cfoot{}
\begin{document}

\section*{Differential entropy} 

\textsl{Differential entropy} is the name given to Shannon's entropy for continuous probablity distributions; the definition is obvious:
\begin{equation}
  h(X)=-\int dx p(x)\log_2{p(x)}
\end{equation}
In other words the only change is to replace the sum over the discrete
variable with an integral over the continuous one. We will see later
however that the relationship between the two definitions, the one we
saw before for discete distributions and this new definition for
continuous distributions, isn't a straight forward as you might hope
and so it makes sense to use a different symbol, in this case a small
$h$ for the differential entropy.

Lets consider the uniform distribution:
\begin{equation}
  p(x)=\le
\end{equation}






\end{document}

